# ğŸ¯ QUICK REFERENCE - MedInSight RAG with Chat UI

## ğŸ“ Complete File Structure
```
/Users/hariprasathc/Hackathon/Hari-version/
â”‚
â”œâ”€â”€ ğŸŒ FRONTEND
â”‚   â””â”€â”€ static/
â”‚       â””â”€â”€ chat.html          â† Beautiful chat interface (NEW!)
â”‚
â”œâ”€â”€ âš™ï¸ BACKEND
â”‚   â”œâ”€â”€ app.py                 â† FastAPI server (UPDATED with chat routes)
â”‚   â””â”€â”€ rag_pipeline.py        â† RAG implementation
â”‚
â”œâ”€â”€ ğŸ“¥ INGESTION
â”‚   â”œâ”€â”€ ingest_simple.py       â† Quick (10 sec, built-in knowledge)
â”‚   â”œâ”€â”€ ingest_dataset.py      â† Full (5-15 min, Dataset PDFs) (NEW!)
â”‚   â””â”€â”€ ingest.py              â† Legacy (for pdfs/ folder)
â”‚
â”œâ”€â”€ ğŸ§ª TESTING
â”‚   â””â”€â”€ test_api.py            â† Automated API tests
â”‚
â”œâ”€â”€ ğŸš€ DEPLOYMENT
â”‚   â”œâ”€â”€ start.sh               â† One-command startup (NEW!)
â”‚   â””â”€â”€ requirements.txt       â† Python dependencies
â”‚
â”œâ”€â”€ ğŸ“– DOCUMENTATION
â”‚   â”œâ”€â”€ README.md              â† Main docs (UPDATED with chat info)
â”‚   â”œâ”€â”€ COMPLETE.md            â† This summary (NEW!)
â”‚   â”œâ”€â”€ RENDER_DEPLOY.md       â† Render deployment guide (NEW!)
â”‚   â”œâ”€â”€ SUBMISSION_GUIDE.md    â† Hack-A-Cure checklist
â”‚   â””â”€â”€ DEPLOYMENT.md          â† General deployment
â”‚
â””â”€â”€ ğŸ“‚ DATA (Generated)
    â”œâ”€â”€ Dataset/               â† Put your PDFs here
    â”œâ”€â”€ vectorstore/           â† Generated FAISS index
    â”‚   â”œâ”€â”€ index.faiss
    â”‚   â””â”€â”€ metadata.pkl
    â””â”€â”€ .env.example           â† API key template
```

---

## ğŸ¯ Routes & Endpoints

| Route | Method | Purpose | Response |
|-------|--------|---------|----------|
| `/` | GET | Root redirect | Redirects to `/query` |
| `/query` | GET | **Chat Interface** ğŸ’¬ | HTML page |
| `/api/query` | POST | API endpoint | JSON |
| `/query` | POST | Legacy API (backward compat) | JSON |
| `/health` | GET | Health check | `{"status": "ok"}` |

---

## âš¡ Quick Commands

### Local Development
```bash
# Setup (one command)
export OPENAI_API_KEY="sk-..." && ./start.sh

# Manual setup
export OPENAI_API_KEY="sk-..."
pip install -r requirements.txt
python ingest_simple.py          # or ingest_dataset.py
python app.py

# Open chat
open http://localhost:10000/query
```

### Render Deployment
```bash
# Environment Variables (in Render dashboard)
OPENAI_API_KEY=sk-your-key-here

# Build Command (choose one)
pip install -r requirements.txt && python ingest_simple.py
pip install -r requirements.txt && python ingest_dataset.py

# Start Command
python app.py
```

### Testing
```bash
# Health check
curl http://localhost:10000/health

# API test
curl -X POST http://localhost:10000/api/query \
  -H "Content-Type: application/json" \
  -d '{"query": "What is diabetes?", "top_k": 3}'

# Automated tests
python test_api.py

# Chat UI
open http://localhost:10000/query
```

---

## ğŸ¨ Chat Interface Features

### Visual Design
- Purple gradient theme
- Clean, modern UI
- Smooth animations
- Mobile-responsive

### Functionality
- Real-time Q&A
- Source citation display
- Sample question buttons
- Loading indicators
- Error handling
- Message history
- Auto-scroll

### User Flow
1. User visits `/query`
2. Sees welcome screen with sample questions
3. Types or clicks a question
4. Clicks "Send"
5. Sees loading spinner
6. Gets answer with source citations
7. Can ask more questions

---

## ğŸ“Š Ingestion Options Comparison

| Feature | ingest_simple.py | ingest_dataset.py |
|---------|------------------|-------------------|
| **Speed** | ~10 seconds | 5-15 minutes |
| **Source** | Built-in knowledge | Dataset PDFs |
| **Chunks** | 20 predefined | 100s-1000s extracted |
| **Memory** | Very low | Medium |
| **Best For** | Quick demo, testing | Production, full data |
| **Deploy Time** | Fast | Slower |

---

## ğŸ”§ Configuration Quick Reference

### Embedding Model
**File**: `rag_pipeline.py`, `ingest_*.py`  
**Current**: `text-embedding-3-small` (1536 dim)  
**Alternative**: `text-embedding-3-large` (3072 dim)

### LLM Model
**File**: `rag_pipeline.py`  
**Current**: `gpt-4o`  
**Note**: GPT-5 not yet available

### Chunking (ingest_dataset.py)
```python
CHUNK_SIZE = 800           # Characters per chunk
CHUNK_OVERLAP = 200        # Overlap
MAX_CHUNKS_PER_PDF = 500   # Limit per PDF
BATCH_SIZE = 20            # Embeddings batch
```

### Port
**File**: `app.py`  
**Default**: 10000  
**Override**: Set `PORT` environment variable

---

## âœ… Pre-Deployment Checklist

### Code Ready
- [x] Chat UI created (`static/chat.html`)
- [x] FastAPI routes updated (`app.py`)
- [x] Dataset ingestion script (`ingest_dataset.py`)
- [x] Documentation complete

### Before Deploy
- [ ] Set `OPENAI_API_KEY` in Render
- [ ] Choose ingestion method (simple or dataset)
- [ ] Set correct build command
- [ ] Set start command: `python app.py`

### After Deploy
- [ ] Check health endpoint works
- [ ] Open chat UI at `/query`
- [ ] Test with sample question
- [ ] Verify source citations appear
- [ ] Test API with curl

---

## ğŸ¯ Hack-A-Cure Submission

### What to Submit
**URL**: `https://your-app.onrender.com`

### What Judges Will See
1. **Chat Interface** at `/query`
   - Professional UI
   - Real-time responses
   - Source citations

2. **API Compliance** at `/api/query`
   - Exact JSON format
   - Fail-safe responses
   - Grounded answers

3. **Documentation**
   - Complete README
   - Deployment guides
   - API examples

### Scoring Advantages
- âœ… Exceeds requirements (chat UI bonus!)
- âœ… Professional presentation
- âœ… Complete documentation
- âœ… Production-ready code
- âœ… Mobile-friendly
- âœ… Anti-hallucination safeguards

---

## ğŸ› Troubleshooting Quick Fixes

| Problem | Solution |
|---------|----------|
| Chat UI blank | Check `static/chat.html` exists |
| API 500 error | Run ingestion, check API key |
| "Vectorstore not found" | Run `ingest_simple.py` |
| Memory error | Reduce `MAX_CHUNKS_PER_PDF` |
| Build timeout | Use `ingest_simple.py` |
| Chat not sending | Check browser console |

---

## ğŸ“± Access Points

### Local
- **Chat**: http://localhost:10000/query
- **API**: http://localhost:10000/api/query
- **Health**: http://localhost:10000/health
- **Docs**: http://localhost:10000/docs (FastAPI auto-docs)

### Production (Render)
- **Chat**: https://your-app.onrender.com/query
- **API**: https://your-app.onrender.com/api/query
- **Health**: https://your-app.onrender.com/health

---

## ğŸ’¡ Pro Tips

1. **First Deploy**: Use `ingest_simple.py` to verify everything works
2. **Then**: Redeploy with `ingest_dataset.py` for full PDFs
3. **Testing**: Use sample questions in chat UI to demo
4. **Customization**: Edit `chat.html` colors/text easily
5. **Monitoring**: Check `/health` endpoint regularly

---

## ğŸ‰ Success Metrics

Your deployment is successful when:
- âœ… `/health` returns `{"status": "ok"}`
- âœ… `/query` loads beautiful chat UI
- âœ… Sample questions work in chat
- âœ… Answers include citations `[1]`, `[2]`
- âœ… API curl test works
- âœ… Mobile browser works

---

## ğŸ“ Quick Help

### File Purposes
- `app.py` â†’ Server routing
- `rag_pipeline.py` â†’ RAG logic
- `chat.html` â†’ Chat UI
- `ingest_*.py` â†’ Data processing
- `*.md` â†’ Documentation

### Where to Look
- **Chat not working?** â†’ Check `app.py` routes
- **API errors?** â†’ Check `rag_pipeline.py`
- **UI issues?** â†’ Edit `static/chat.html`
- **Deploy issues?** â†’ Read `RENDER_DEPLOY.md`
- **General help?** â†’ Read `README.md`

---

## ğŸš€ Final Command Sequence

```bash
# 1. Set API key
export OPENAI_API_KEY="sk-..."

# 2. Install
pip install -r requirements.txt

# 3. Ingest (choose one)
python ingest_simple.py

# 4. Start
python app.py

# 5. Open browser
open http://localhost:10000/query

# 6. Test
curl http://localhost:10000/health
```

---

**You're ready to deploy and submit! ğŸ‰**

*This is your complete, production-ready RAG system with chat interface!*
